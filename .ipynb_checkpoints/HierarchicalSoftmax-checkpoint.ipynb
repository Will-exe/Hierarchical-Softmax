{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efaf438e",
   "metadata": {},
   "source": [
    "# Hierarchical Softmax\n",
    "\n",
    "## Introduction: Classification in Computer Vision\n",
    "\n",
    "Classification is at the heart of supervised AI, enabling machines to make sense of visual data by categorizing images or videos into meaningful labels. Tasks range from **binary classification**, where the model determines one of two outcomes (e.g., \"cat\" vs. \"not a cat\"), to **multi-label classification**, where multiple labels can apply (e.g., \"dog\" and \"outdoor\"). As datasets grow, managing large label spaces efficiently becomes critical. This is where **hierarchical softmax** shines, offering a scalable solution for high-dimensional classification problems.\n",
    "\n",
    "\n",
    "## Classical approaches\n",
    "\n",
    "\n",
    "### Binary Classification\n",
    "\n",
    "\n",
    "Binary classification is a supervised learning task where a model predicts one of two possible outcomes. In neural networks, this is often achieved using the **sigmoid activation function**, which maps the model's output to a probability value between 0 and 1. \n",
    "\n",
    "The sigmoid function is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Here, \\(z\\) represents the model's raw output (inlogits). The function is a simple exponential, effectively compressing the result into a range of 0 to 1, where 1 represents a positive prediction, and 0 a negative prediction. For example, if the model predicts a large positive number, the sigmoid function will output a value close to **1**, indicating a high probability for positive class 1. On the other hand, a large negative number results in a value close to **0**, indicating a high probability of negative class 0.\n",
    "\n",
    "\n",
    "### **Multi-Class Classification**\n",
    "\n",
    "\n",
    "When talking about multi classification, there are two main types to consider:\n",
    "\n",
    "In **multi-head classification**, each \"head\" (or output node) corresponds to a specific class, and the model learns to predict a separate probability distribution for each class. This setup is often used in tasks like **multi-label classification**, where multiple labels can apply to a single input, such as detecting multiple objects in an image.\n",
    "\n",
    "To make this work, models often use sigmoid activation for each head, as each class prediction is independent, allowing the model to output probabilities for all classes at once.\n",
    "\n",
    "In **multi-class classification** however, a model predicts a single label from a set of multiple possible labels. Multi-class classification handles more complex problems, such as identifying which animal is in a photo from a set of species. Typically, **softmax** is used to convert the model's raw outputs into probabilities, ensuring the label with the highest probability is selected as the predicted class.\n",
    "\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20240105124903/Mutliclass-Classification-vs-multilabel-classification-(1).png\" alt=\"Visual Difference between Multihead vs Multiclass\" width=\"400\" height=\"275\">\n",
    "\n",
    "\n",
    "### The Softmax Equation\n",
    "\n",
    "**Softmax** is an activation function used in problems and is a generalization of the sigmoid function, applied to higher dimensions. Softmax converts a vector of raw model outputs (also logits) into a probability distribution across multiple classes. The sum of these probabilities will always equal 1, making it ideal for predicting which class an input belongs to when there are more than two options.\n",
    "\n",
    "The softmax function for a given class \\(i\\) is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\($z_i$\\) is the raw output (logit) for class \\(i\\),\n",
    "-  \\($e^{z_i}$\\)  is the exponential of that logit,\n",
    "- The denominator sums the exponentials of all logits \\($z_j$\\) for all classes.\n",
    "\n",
    "The softmax function ensures that the class with the highest probability is selected, while considering all other classes' relative probabilities. Unlike multi-label classification, where multiple labels can be predicted independently, softmax forces the model to select the most probable class from a set of mutually exclusive classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b43068",
   "metadata": {},
   "source": [
    "\n",
    "While softmax is effective in multi-class classification, it has some limitations when dealing with large datasets or a high number of classes. As the number of classes increases, the computational cost of calculating softmax is O(n), thus it requires a hefty computational cost at high dimensions. This makes it inefficient for problems with vast label spaces, as it can lead to longer training times and higher memory usage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2165d1",
   "metadata": {},
   "source": [
    "## Hierarchical Softmax\n",
    "\n",
    "### Origin\n",
    "\n",
    "**Hierarchical softmax** is an approximate method inspired by binary trees that was first proposed by Morin and\n",
    "Bengio. It was then introduced by **Mikolov et al.** in the context of training neural network-based language models, where calculating softmax for a vast number of classes became computationally expensive. By organizing classes into a binary tree structure, hierarchical softmax reduces the computational complexity from \\($O(N)$\\) to \\($O(\\log N)$\\), significantly improving the efficiency of training on large-scale datasets.\n",
    "\n",
    "Instead of calculating the softmax across all classes, it organizes the classes into a binary tree structure, where each class is a leaf node.\n",
    "\n",
    "### The Definition\n",
    "\n",
    "In a standard softmax, we compute:\n",
    "\n",
    "$$\n",
    "P(y = c | x) = \\frac{e^{w_c \\cdot x}}{\\sum_{c'} e^{w_{c'} \\cdot x}}\n",
    "$$\n",
    "\n",
    "where \\($w_c$\\) represents the weight for class \\($c$\\), and \\($x$\\) is the input.\n",
    "\n",
    "In hierarchical softmax, we represent the prediction as a series of binary decisions to traverse the tree. Each internal node in the tree makes a binary decision using a logistic function (sigmoid). For a binary tree with nodes \\($n_1, n_2, ..., n_K$\\) leading to class \\(c\\), the probability of class \\(c\\) is computed as:\n",
    "\n",
    "$$\n",
    "P(y = c | x) = \\prod_{k=1}^{K} \\sigma(n_k \\cdot x)\n",
    "$$\n",
    "\n",
    "where \\($\\sigma$\\) is the sigmoid function applied at each node, and each \\($n_k$\\) corresponds to a binary decision along the path to class \\(c\\). \n",
    "\n",
    "Instead of calculating the softmax across all classes, it organizes the classes into a binary tree structure, where each class is a leaf node. For each of these leaf nodes, there exists a unique path from the root of the tree to the node, and this path is what is used to estimate the probability of the label (or usually word) represented by the leaf node.\n",
    "\n",
    "<img src=\"https://cdn.discordapp.com/attachments/1317282175942656000/1317333535035752524/image.png?ex=675e4df5&is=675cfc75&hm=54cf876547225cc81fa1e37e96ab045b78b9054c7bd95c79c1e86bfaa6e40663&\" alt=\"Visual Difference between Multihead vs Multiclass\" width=\"400\" height=\"275\">\n",
    "\n",
    "In this example, the white nodes represent words in the target vocabulary, while the dark nodes are internal units. The highlighted path is the traverse sequence from the root to the word $w_2$.\n",
    "\n",
    "The binary trees used in hierarchical softmax, as evident in the example, always have K words, and K-1 inner units\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c6aacf",
   "metadata": {},
   "source": [
    "### Intuition\n",
    "\n",
    "To understand the equation, let's go through the given example. Suppose we want to compute the probability of the word $w_2$ being the output. This probability is represented as the likelihood of a random walk starting from the root and ending at the leaf corresponding to $w_2$\n",
    "\n",
    "At each inner node, we decide whether to go left or right, with the probability of going left at an inner unit \\($n$\\) given by:\n",
    "\n",
    "$$ P(n, \\text{left} ) = \\sigma({v'_n}^T \\cdot h) $$\n",
    "\n",
    "Where \\($v'_n$\\) is the vector representation of the inner node and \\($h$\\) is the hidden layer's output.\n",
    "\n",
    "The probability of going right at unit \\($n$\\) is:\n",
    "\n",
    "$$\n",
    "P(n, \\text{right}) = 1 - \\sigma({v'_n}^T \\cdot h) = \\sigma(-{v'_n}^T \\cdot h)\n",
    "$$\n",
    "\n",
    "To compute the probability of $w_2$ being the output word, we multiply the probabilities along the path from the root to $w_2$:\n",
    "\n",
    "$$\n",
    "p(w2 = w_O) = p(n(w_2, 1), \\text{left}) \\cdot p(n(w_2, 2), \\text{left}) \\cdot p(n(w_2, 3), \\text{right})\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sigma({v'_{n(w_2, 1)}}^T h) \\cdot \\sigma({v'_{n(w_2, 2)}}^T h) \\cdot \\sigma(-{v'_{n(w_2, 3)}}^T h)\n",
    "$$\n",
    "\n",
    "This is the core of the hierarchical softmax \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f5037a",
   "metadata": {},
   "source": [
    "### Cons of Hierarchical Softmax\n",
    "\n",
    "The performance of a model using hierarchical softmax is highly dependent on the structure of the tree: A well-constructed tree leads to better training, while a poorly structured tree may hinder the model’s learning process. Given ambiguous grouping, where words aren't grouped logically, may not be able to select the correct path during inference.\n",
    "\n",
    "In order to generate a good tree, words are grouped logically by their meanings. For example, words related to sports are on one side of the tree, while words related to musical instruments are on the other. This organization helps the model learn relationships between words efficiently. \n",
    "\n",
    "### Huffman Tree\n",
    "\n",
    "When building the tree, we can favor words with high frequency such that they are placed closer to the root, while placing rare words deeper in the tree. This is an example of a **Huffman tree**. This structure helps the model make predictions faster, especially in large datasets with many classes, by minimizing the computational cost of the softmax calculation. In practice, this also has the benefit of better logical grouping of words, helping inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f22fa7",
   "metadata": {},
   "source": [
    "### Performance\n",
    "\n",
    "#### How does Hierarchical Softmax compare to Softmax?\n",
    "\n",
    "<img src=\"https://cdn.discordapp.com/attachments/1317282175942656000/1317344309737160844/image.png?ex=675e57fe&is=675d067e&hm=499d3f76b633777e954809ad2a075e75b92713e88b754e2ea7df4ee3acfde555&\" alt=\"Accuracy and Loss function performance of Softmax vs Hierarchical Softmax\" width=\"800\" height=\"600\">\n",
    "Accuracy and Loss function performance of Softmax vs Hierarchical Softmax ( https://doi-org.uproxy.library.dc-uoit.ca/10.1002/ett.3920 )\n",
    "\n",
    "As seen in this example where the different Softmaxes are used to differentiate between malware and benign samples, Hierarchical Softmax outperforms softmax only with little training. Given enough resources and training, softmax can be expected to yield a higher accuracy.\n",
    "\n",
    "This is expected, since Hierarchical Softmax only predicts the true probability when traversing it's tree, whereas Softmax computes the exact probability over all classes. The Hierarchical Softmax also suffers from low convergence rate compared to softmax, possibly requiring more iterations to complete training.\n",
    "\n",
    "That being said, hierarchical softmax provides a significant speedup, especially when the number of classes is large, by leveraging the tree structure to reduce the number of operations required for each prediction. In practice, the trade-off between computational efficiency and accuracy is often favorable for large-scale problems, where the reduction in time complexity provided by hierarchical softmax outweighs the small accuracy difference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7ce450",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "\n",
    "Morin, F. and Bengio, Y. (2005). Hierarchical probabilistic neural network language model.\n",
    "In AISTATS, volume 5, pages 246–252. Citeseer\n",
    "\n",
    "[Multiclass vs multilabel](https://www.geeksforgeeks.org/multiclass-classification-vs-multi-label-classification/)\n",
    "\n",
    "Wang F, Yang S, Li Q, Wang C. An internet of things malware classification method based on mixture of experts neural network. Trans Emerging Tel Tech. 2021; 32:e3920. https://doi-org.uproxy.library.dc-uoit.ca/10.1002/ett.3920\n",
    "\n",
    "[Xin Rong word2vec Parameter Learning Explained](https://arxiv.org/pdf/1411.2738v4)\n",
    "\n",
    "[Lei Mao Hierarchical Softmax](https://leimao.github.io/article/Hierarchical-Softmax/)\n",
    "\n",
    "[Math and visual representation of sigmoid and softmax](https://towardsdatascience.com/a-visual-understanding-of-the-softmax-function-b4d92fdaccfa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
